{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fluF3_oOgkWF"
   },
   "source": [
    "##### Copyright 2020 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2022-09-03T01:21:29.621993Z",
     "iopub.status.busy": "2022-09-03T01:21:29.621418Z",
     "iopub.status.idle": "2022-09-03T01:21:29.625318Z",
     "shell.execute_reply": "2022-09-03T01:21:29.624740Z"
    },
    "id": "AJs7HHFmg1M9"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYysdyb-CaWM"
   },
   "source": [
    "# Simple audio recognition: Recognizing keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNbqmZy0gbyE"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/audio/simple_audio\">\n",
    "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
    "    View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/audio/simple_audio.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/audio/simple_audio.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/audio/simple_audio.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPfDNFlb66XF"
   },
   "source": [
    "This tutorial demonstrates how to preprocess audio files in the WAV format and build and train a basic <a href=\"https://en.wikipedia.org/wiki/Speech_recognition\" class=\"external\">automatic speech recognition</a> (ASR) model for recognizing ten different words. You will use a portion of the [Speech Commands dataset](https://www.tensorflow.org/datasets/catalog/speech_commands) (<a href=\"https://arxiv.org/abs/1804.03209\" class=\"external\">Warden, 2018</a>), which contains short (one-second or less) audio clips of commands, such as \"down\", \"go\", \"left\", \"no\", \"right\", \"stop\", \"up\" and \"yes\".\n",
    "\n",
    "Real-world speech and audio recognition <a href=\"https://ai.googleblog.com/search/label/Speech%20Recognition\" class=\"external\">systems</a> are complex. But, like [image classification with the MNIST dataset](../quickstart/beginner.ipynb), this tutorial should give you a basic understanding of the techniques involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Go9C3uLL8Izc"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Import necessary modules and dependencies. Note that you'll be using <a href=\"https://seaborn.pydata.org/\" class=\"external\">seaborn</a> for visualization in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-03T01:21:29.629217Z",
     "iopub.status.busy": "2022-09-03T01:21:29.628751Z",
     "iopub.status.idle": "2022-09-03T01:21:31.296661Z",
     "shell.execute_reply": "2022-09-03T01:21:31.295871Z"
    },
    "id": "hhNW45sjDEDe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Upgrading Colab's tensorflow breaks GPU support. Switch this notebook back to GPU once Colab upgrades.\n",
    "!pip install -U --pre tensorflow tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-03T01:21:31.300881Z",
     "iopub.status.busy": "2022-09-03T01:21:31.300641Z",
     "iopub.status.idle": "2022-09-03T01:21:34.148138Z",
     "shell.execute_reply": "2022-09-03T01:21:34.147494Z"
    },
    "id": "dzLKpmZICaWN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from IPython import display\n",
    "\n",
    "# Set the seed value for experiment reproducibility.\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yR0EdgrLCaWR"
   },
   "source": [
    "## Import the mini Speech Commands dataset\n",
    "\n",
    "To save time with data loading, you will be working with a smaller version of the Speech Commands dataset. The [original dataset](https://www.tensorflow.org/datasets/catalog/speech_commands) consists of over 105,000 audio files in the <a href=\"https://www.aelius.com/njh/wavemetatools/doc/riffmci.pdf\" class=\"external\">WAV (Waveform) audio file format</a> of people saying 35 different words. This data was collected by Google and released under a CC BY license.\n",
    "\n",
    "Download and extract the `mini_speech_commands.zip` file containing the smaller Speech Commands datasets with `tf.keras.utils.get_file`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-03T01:21:34.152321Z",
     "iopub.status.busy": "2022-09-03T01:21:34.151939Z",
     "iopub.status.idle": "2022-09-03T01:21:38.073312Z",
     "shell.execute_reply": "2022-09-03T01:21:38.072645Z"
    },
    "id": "2-rayb7-3Y0I"
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = 'data/mini_speech_commands'\n",
    "\n",
    "data_dir = pathlib.Path(DATASET_PATH)\n",
    "if not data_dir.exists():\n",
    "  tf.keras.utils.get_file(\n",
    "      'mini_speech_commands.zip',\n",
    "      origin=\"http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip\",\n",
    "      extract=True,\n",
    "      cache_dir='.', cache_subdir='data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgvFq3uYiS5G"
   },
   "source": [
    "The dataset's audio clips are stored in eight folders corresponding to each speech command: `no`, `yes`, `down`, `go`, `left`, `up`, `right`, and `stop`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-03T01:21:38.078122Z",
     "iopub.status.busy": "2022-09-03T01:21:38.077530Z",
     "iopub.status.idle": "2022-09-03T01:21:38.081676Z",
     "shell.execute_reply": "2022-09-03T01:21:38.081112Z"
    },
    "id": "70IBxSKxA1N9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commands: ['down' 'go' 'left' 'no' 'right' 'stop' 'up' 'yes']\n"
     ]
    }
   ],
   "source": [
    "commands = np.array(tf.io.gfile.listdir(str(data_dir)))\n",
    "commands = commands[commands != 'README.md']\n",
    "print('Commands:', commands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZ7GJjDvHqtt"
   },
   "source": [
    "Divided into directories this way, you can easily load the data using `keras.utils.audio_dataset_from_directory`. \n",
    "\n",
    "The audio clips are 1 second or less at 16kHz. The `output_sequence_length=16000` pads the short ones to exactly 1 second (and would trim longer ones) so that they can be easily batched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-03T01:21:38.084902Z",
     "iopub.status.busy": "2022-09-03T01:21:38.084399Z",
     "iopub.status.idle": "2022-09-03T01:21:41.561731Z",
     "shell.execute_reply": "2022-09-03T01:21:41.561097Z"
    },
    "id": "mFM4c3aMC8Qv"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.api._v2.keras.utils' has no attribute 'audio_dataset_from_directory'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_ds, val_ds \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio_dataset_from_directory\u001b[49m(\n\u001b[0;32m      2\u001b[0m     directory\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[0;32m      3\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[0;32m      4\u001b[0m     validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[0;32m      5\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m      6\u001b[0m     output_sequence_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16000\u001b[39m,\n\u001b[0;32m      7\u001b[0m     subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m label_names \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(train_ds\u001b[38;5;241m.\u001b[39mclass_names)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras.api._v2.keras.utils' has no attribute 'audio_dataset_from_directory'"
     ]
    }
   ],
   "source": [
    "train_ds, val_ds = tf.keras.utils.audio_dataset_from_directory(\n",
    "    directory=data_dir,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    seed=0,\n",
    "    output_sequence_length=16000,\n",
    "    subset='both')\n",
    "\n",
    "label_names = np.array(train_ds.class_names)\n",
    "print()\n",
    "print(\"label names:\", label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cestp83qFnU5"
   },
   "source": [
    "The dataset now contains batches of audio clips and integer labels. The audio clips have a shape of `(batch, samples, channels)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-03T01:21:41.565005Z",
     "iopub.status.busy": "2022-09-03T01:21:41.564780Z",
     "iopub.status.idle": "2022-09-03T01:21:41.573296Z",
     "shell.execute_reply": "2022-09-03T01:21:41.572758Z"
    },
    "id": "3yU6SQGIFb3H"
   },
   "outputs": [],
   "source": [
    "train_ds.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppG9Dgq2Ex8R"
   },
   "source": [
    "This dataset only contains single channel audio, so use the `tf.squeeze` function to drop the extra axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-03T01:21:41.576707Z",
     "iopub.status.busy": "2022-09-03T01:21:41.576137Z",
     "iopub.status.idle": "2022-09-03T01:21:41.612310Z",
     "shell.execute_reply": "2022-09-03T01:21:41.611786Z"
    },
    "id": "Xl-tnniUIBlM"
   },
   "outputs": [],
   "source": [
    "def squeeze(audio, labels):\n",
    "  audio = tf.squeeze(audio, axis=-1)\n",
    "  return audio, labels\n",
    "\n",
    "train_ds = train_ds.map(squeeze, tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.map(squeeze, tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtsCSWZN5ILv"
   },
   "source": [
    "The `utils.audio_dataset_from_directory` function only returns up to two splits. It's a good idea to keep a test set separate from your validation set.\n",
    "Ideally you'd keep it in a separate directory, but in this case you can use `Dataset.shard` to split the validation set into two halves. Note that iterating over **any** shard will load **all** the data, and only keep it's fraction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-03T01:21:41.615802Z",
     "iopub.status.busy": "2022-09-03T01:21:41.615350Z",
     "iopub.status.idle": "2022-09-03T01:21:41.620445Z",
     "shell.execute_reply": "2022-09-03T01:21:41.619930Z"
    },
    "id": "u5UEGsqM5Gss"
   },
   "outputs": [],
   "source": [
    "test_ds = val_ds.shard(num_shards=2, index=0)\n",
    "val_ds = val_ds.shard(num_shards=2, index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-03T01:21:41.623394Z",
     "iopub.status.busy": "2022-09-03T01:21:41.622963Z",
     "iopub.status.idle": "2022-09-03T01:21:41.718192Z",
     "shell.execute_reply": "2022-09-03T01:21:41.717418Z"
    },
    "id": "xIeoJcwJH5h9"
   },
   "outputs": [],
   "source": [
    "for example_audio, example_labels in train_ds.take(1):  \n",
    "  print(example_audio.shape)\n",
    "  print(example_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voxGEwvuh2L7"
   },
   "source": [
    "Let's plot a few audio waveforms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-03T01:21:41.722134Z",
     "iopub.status.busy": "2022-09-03T01:21:41.721510Z",
     "iopub.status.idle": "2022-09-03T01:21:41.726238Z",
     "shell.execute_reply": "2022-09-03T01:21:41.725620Z"
    },
    "id": "dYtGq2zYNHuT"
   },
   "outputs": [],
   "source": [
    "label_names[[1,1,3,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-03T01:21:41.729036Z",
     "iopub.status.busy": "2022-09-03T01:21:41.728641Z",
     "iopub.status.idle": "2022-09-03T01:21:42.666892Z",
     "shell.execute_reply": "2022-09-03T01:21:42.666209Z"
    },
    "id": "8yuX6Nqzf6wT"
   },
   "outputs": [],
   "source": [
    "rows = 3\n",
    "cols = 3\n",
    "n = rows * cols\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(16, 9))\n",
    "\n",
    "for i in range(n):\n",
    "  if i>=n:\n",
    "    break\n",
    "  r = i // cols\n",
    "  c = i % cols\n",
    "  ax = axes[r][c]\n",
    "  ax.plot(example_audio[i].numpy())\n",
    "  ax.set_yticks(np.arange(-1.2, 1.2, 0.2))\n",
    "  label = label_names[example_labels[i]]\n",
    "  ax.set_title(label)\n",
    "  ax.set_ylim([-1.1,1.1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWXPphxm0B4m"
   },
   "source": [
    "## Convert waveforms to spectrograms\n",
    "\n",
    "The waveforms in the dataset are represented in the time domain. Next, you'll transform the waveforms from the time-domain signals into the time-frequency-domain signals by computing the <a href=\"https://en.wikipedia.org/wiki/Short-time_Fourier_transform\" class=\"external\">short-time Fourier transform (STFT)</a> to convert the waveforms to as <a href=\"https://en.wikipedia.org/wiki/Spectrogram\" clas=\"external\">spectrograms</a>, which show frequency changes over time and can be represented as 2D images. You will feed the spectrogram images into your neural network to train the model.\n",
    "\n",
    "A Fourier transform (`tf.signal.fft`) converts a signal to its component frequencies, but loses all time information. In comparison, STFT (`tf.signal.stft`) splits the signal into windows of time and runs a Fourier transform on each window, preserving some time information, and returning a 2D tensor that you can run standard convolutions on.\n",
    "\n",
    "Create a utility function for converting waveforms to spectrograms:\n",
    "\n",
    "- The waveforms need to be of the same length, so that when you convert them to spectrograms, the results have similar dimensions. This can be done by simply zero-padding the audio clips that are shorter than one second (using `tf.zeros`).\n",
    "- When calling `tf.signal.stft`, choose the `frame_length` and `frame_step` parameters such that the generated spectrogram \"image\" is almost square. For more information on the STFT parameters choice, refer to <a href=\"https://www.coursera.org/lecture/audio-signal-processing/stft-2-tjEQe\" class=\"external\">this Coursera video</a> on audio signal processing and STFT.\n",
    "- The STFT produces an array of complex numbers representing magnitude and phase. However, in this tutorial you'll only use the magnitude, which you can derive by applying `tf.abs` on the output of `tf.signal.stft`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-03T01:21:42.672011Z",
     "iopub.status.busy": "2022-09-03T01:21:42.671398Z",
     "iopub.status.idle": "2022-09-03T01:21:42.675516Z",
     "shell.execute_reply": "2022-09-03T01:21:42.674936Z"
    },
    "id": "_4CK75DHz_OR"
   },
   "outputs": [],
   "source": [
    "def get_spectrogram(waveform):\n",
    "  # Convert the waveform to a spectrogram via a STFT.\n",
    "  spectrogram = tf.signal.stft(\n",
    "      waveform, frame_length=255, frame_step=128)\n",
    "  # Obtain the magnitude of the STFT.\n",
    "  spectrogram = tf.abs(spectrogram)\n",
    "  # Add a `channels` dimension, so that the spectrogram can be used\n",
    "  # as image-like input data with convolution layers (which expect\n",
    "  # shape (`batch_size`, `height`, `width`, `channels`).\n",
    "  spectrogram = spectrogram[..., tf.newaxis]\n",
    "  return spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rdPiPYJphs2"
   },
   "source": [
    "Next, start exploring the data. Print the shapes of one example's tensorized waveform and the corresponding spectrogram, and play the original audio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-03T01:21:42.678876Z",
     "iopub.status.busy": "2022-09-03T01:21:42.678491Z",
     "iopub.status.idle": "2022-09-03T01:21:42.897185Z",
     "shell.execute_reply": "2022-09-03T01:21:42.896505Z"
    },
    "id": "4Mu6Y7Yz3C-V"
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "  label = label_names[example_labels[i]]\n",
    "  waveform = example_audio[i]\n",
    "  spectrogram = get_spectrogram(waveform)\n",
    "\n",
    "  print('Label:', label)\n",
    "  print('Waveform shape:', waveform.shape)\n",
    "  print('Spectrogram shape:', spectrogram.shape)\n",
    "  print('Audio playback')\n",
    "  display.display(display.Audio(waveform, rate=16000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnSuqyxJ1isF"
   },
   "source": [
    "Now, define a function for displaying a spectrogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-03T01:21:42.901148Z",
     "iopub.status.busy": "2022-09-03T01:21:42.900541Z",
     "iopub.status.idle": "2022-09-03T01:21:42.905351Z",
     "shell.execute_reply": "2022-09-03T01:21:42.904771Z"
    },
    "id": "e62jzb36-Jog"
   },
   "outputs": [],
   "source": [
    "def plot_spectrogram(spectrogram, ax):\n",
    "  if len(spectrogram.shape) > 2:\n",
    "    assert len(spectrogram.shape) == 3\n",
    "    spectrogram = np.squeeze(spectrogram, axis=-1)\n",
    "  # Convert the frequencies to log scale and transpose, so that the time is\n",
    "  # represented on the x-axis (columns).\n",
    "  # Add an epsilon to avoid taking a log of zero.\n",
    "  log_spec = np.log(spectrogram.T + np.finfo(float).eps)\n",
    "  height = log_spec.shape[0]\n",
    "  width = log_spec.shape[1]\n",
    "  X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n",
    "  Y = range(height)\n",
    "  ax.pcolormesh(X, Y, log_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baa5c91e8603"
   },
   "source": [
    "Plot the example's waveform over time and the corresponding spectrogram (frequencies over time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-03T01:21:42.909109Z",
     "iopub.status.busy": "2022-09-03T01:21:42.908629Z",
     "iopub.status.idle": "2022-09-03T01:21:43.143210Z",
     "shell.execute_reply": "2022-09-03T01:21:43.142639Z"
    },
    "id": "d2_CikgY1tjv"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, figsize=(12, 8))\n",
    "timescale = np.arange(waveform.shape[0])\n",
    "axes[0].plot(timescale, waveform.numpy())\n",
    "axes[0].set_title('Waveform')\n",
    "axes[0].set_xlim([0, 16000])\n",
    "\n",
    "plot_spectrogram(spectrogram.numpy(), axes[1])\n",
    "axes[1].set_title('Spectrogram')\n",
    "plt.suptitle(label.title())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyYXjW07jCHA"
   },
   "source": [
    "Now, create spectrogramn datasets from the audio datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-03T01:21:43.148164Z",
     "iopub.status.busy": "2022-09-03T01:21:43.147708Z",
     "iopub.status.idle": "2022-09-03T01:21:43.151088Z",
     "shell.execute_reply": "2022-09-03T01:21:43.150558Z"
    },
    "id": "mAD0LpkgqtQo"
   },
   "outputs": [],
   "source": [
    "def make_spec_ds(ds):\n",
    "  return ds.map(\n",
    "      map_func=lambda audio,label: (get_spectrogram(audio), label),\n",
    "      num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-03T01:21:43.154026Z",
     "iopub.status.busy": "2022-09-03T01:21:43.153520Z",
     "iopub.status.idle": "2022-09-03T01:21:43.381041Z",
     "shell.execute_reply": "2022-09-03T01:21:43.380439Z"
    },
    "id": "yEVb_oK0oBLQ"
   },
   "outputs": [],
   "source": [
    "train_spectrogram_ds = make_spec_ds(train_ds)\n",
    "val_spectrogram_ds = make_spec_ds(val_ds)\n",
    "test_spectrogram_ds = make_spec_ds(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6gQpAAgMnyDi"
   },
   "source": [
    "Examine the spectrograms for different examples of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-03T01:21:43.385041Z",
     "iopub.status.busy": "2022-09-03T01:21:43.384465Z",
     "iopub.status.idle": "2022-09-03T01:21:43.673215Z",
     "shell.execute_reply": "2022-09-03T01:21:43.672518Z"
    },
    "id": "EaM2q5aGis-d"
   },
   "outputs": [],
   "source": [
    "for example_spectrograms, example_spect_labels in train_spectrogram_ds.take(1):\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-03T01:21:43.677266Z",
     "iopub.status.busy": "2022-09-03T01:21:43.676740Z",
     "iopub.status.idle": "2022-09-03T01:21:44.475999Z",
     "shell.execute_reply": "2022-09-03T01:21:44.475386Z"
    },
    "id": "QUbHfTuon4iF"
   },
   "outputs": [],
   "source": [
    "rows = 3\n",
    "cols = 3\n",
    "n = rows*cols\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(16, 9))\n",
    "\n",
    "for i in range(n):\n",
    "    r = i // cols\n",
    "    c = i % cols\n",
    "    ax = axes[r][c]\n",
    "    plot_spectrogram(example_spectrograms[i].numpy(), ax)\n",
    "    ax.set_title(commands[example_spect_labels[i].numpy()])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5KdY8IF8rkt"
   },
   "source": [
    "## Build and train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GS1uIh6F_TN9"
   },
   "source": [
    "Add `Dataset.cache` and `Dataset.prefetch` operations to reduce read latency while training the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-03T01:21:44.485927Z",
     "iopub.status.busy": "2022-09-03T01:21:44.485677Z",
     "iopub.status.idle": "2022-09-03T01:21:44.497077Z",
     "shell.execute_reply": "2022-09-03T01:21:44.496451Z"
    },
    "id": "fdZ6M-F5_QzY"
   },
   "outputs": [],
   "source": [
    "train_spectrogram_ds = train_spectrogram_ds.cache().shuffle(10000).prefetch(tf.data.AUTOTUNE)\n",
    "val_spectrogram_ds = val_spectrogram_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
    "test_spectrogram_ds = test_spectrogram_ds.cache().prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwHkKCQQb5oW"
   },
   "source": [
    "For the model, you'll use a simple convolutional neural network (CNN), since you have transformed the audio files into spectrogram images.\n",
    "\n",
    "Your `tf.keras.Sequential` model will use the following Keras preprocessing layers:\n",
    "\n",
    "- `tf.keras.layers.Resizing`: to downsample the input to enable the model to train faster.\n",
    "- `tf.keras.layers.Normalization`: to normalize each pixel in the image based on its mean and standard deviation.\n",
    "\n",
    "For the `Normalization` layer, its `adapt` method would first need to be called on the training data in order to compute aggregate statistics (that is, the mean and the standard deviation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-03T01:21:44.500526Z",
     "iopub.status.busy": "2022-09-03T01:21:44.500085Z",
     "iopub.status.idle": "2022-09-03T01:21:45.669574Z",
     "shell.execute_reply": "2022-09-03T01:21:45.668960Z"
    },
    "id": "ALYz7PFCHblP"
   },
   "outputs": [],
   "source": [
    "input_shape = example_spectrograms.shape[1:]\n",
    "print('Input shape:', input_shape)\n",
    "num_labels = len(commands)\n",
    "\n",
    "# Instantiate the `tf.keras.layers.Normalization` layer.\n",
    "norm_layer = layers.Normalization()\n",
    "# Fit the state of the layer to the spectrograms\n",
    "# with `Normalization.adapt`.\n",
    "norm_layer.adapt(data=train_spectrogram_ds.map(map_func=lambda spec, label: spec))\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    # Downsample the input.\n",
    "    layers.Resizing(32, 32),\n",
    "    # Normalize.\n",
    "    norm_layer,\n",
    "    layers.Conv2D(32, 3, activation='relu'),\n",
    "    layers.Conv2D(64, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_labels),\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de52e5afa2f3"
   },
   "source": [
    "Configure the Keras model with the Adam optimizer and the cross-entropy loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-03T01:21:45.681110Z",
     "iopub.status.busy": "2022-09-03T01:21:45.680631Z",
     "iopub.status.idle": "2022-09-03T01:21:45.691616Z",
     "shell.execute_reply": "2022-09-03T01:21:45.691064Z"
    },
    "id": "wFjj7-EmsTD-"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f42b9e3a4705"
   },
   "source": [
    "Train the model over 10 epochs for demonstration purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-03T01:21:45.695324Z",
     "iopub.status.busy": "2022-09-03T01:21:45.694898Z",
     "iopub.status.idle": "2022-09-03T01:21:56.843528Z",
     "shell.execute_reply": "2022-09-03T01:21:56.842866Z"
    },
    "id": "ttioPJVMcGtq"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "history = model.fit(\n",
    "    train_spectrogram_ds,\n",
    "    validation_data=val_spectrogram_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjpCDeQ4mUfS"
   },
   "source": [
    "Let's plot the training and validation loss curves to check how your model has improved during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-03T01:21:56.847456Z",
     "iopub.status.busy": "2022-09-03T01:21:56.846909Z",
     "iopub.status.idle": "2022-09-03T01:21:57.240892Z",
     "shell.execute_reply": "2022-09-03T01:21:57.240260Z"
    },
    "id": "nzhipg3Gu2AY"
   },
   "outputs": [],
   "source": [
    "metrics = history.history\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.ylim([0, max(plt.ylim())])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss [CrossEntropy]')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.epoch, 100*np.array(metrics['accuracy']), 100*np.array(metrics['val_accuracy']))\n",
    "plt.legend(['accuracy', 'val_accuracy'])\n",
    "plt.ylim([0, 100])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy [%]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZTt3kO3mfm4"
   },
   "source": [
    "## Evaluate the model performance\n",
    "\n",
    "Run the model on the test set and check the model's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-03T01:21:57.244818Z",
     "iopub.status.busy": "2022-09-03T01:21:57.244571Z",
     "iopub.status.idle": "2022-09-03T01:21:57.534692Z",
     "shell.execute_reply": "2022-09-03T01:21:57.533977Z"
    },
    "id": "FapuRT_SsWGQ"
   },
   "outputs": [],
   "source": [
    "model.evaluate(test_spectrogram_ds, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "en9Znt1NOabH"
   },
   "source": [
    "### Display a confusion matrix\n",
    "\n",
    "Use a <a href=\"https://developers.google.com/machine-learning/glossary#confusion-matrix\" class=\"external\">confusion matrix</a> to check how well the model did classifying each of the commands in the test set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-03T01:21:57.538525Z",
     "iopub.status.busy": "2022-09-03T01:21:57.537924Z",
     "iopub.status.idle": "2022-09-03T01:21:57.674686Z",
     "shell.execute_reply": "2022-09-03T01:21:57.673977Z"
    },
    "id": "5Y6vmWWQuuT1"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_spectrogram_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-03T01:21:57.678141Z",
     "iopub.status.busy": "2022-09-03T01:21:57.677627Z",
     "iopub.status.idle": "2022-09-03T01:21:57.682371Z",
     "shell.execute_reply": "2022-09-03T01:21:57.681749Z"
    },
    "id": "d6F0il82u7lW"
   },
   "outputs": [],
   "source": [
    "y_pred = tf.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-03T01:21:57.685164Z",
     "iopub.status.busy": "2022-09-03T01:21:57.684956Z",
     "iopub.status.idle": "2022-09-03T01:21:57.731134Z",
     "shell.execute_reply": "2022-09-03T01:21:57.730537Z"
    },
    "id": "vHSNoBYLvX81"
   },
   "outputs": [],
   "source": [
    "y_true = tf.concat(list(test_spectrogram_ds.map(lambda s,lab: lab)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-03T01:21:57.734548Z",
     "iopub.status.busy": "2022-09-03T01:21:57.734100Z",
     "iopub.status.idle": "2022-09-03T01:21:58.077802Z",
     "shell.execute_reply": "2022-09-03T01:21:58.077148Z"
    },
    "id": "LvoSAOiXU3lL"
   },
   "outputs": [],
   "source": [
    "confusion_mtx = tf.math.confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_mtx,\n",
    "            xticklabels=commands,\n",
    "            yticklabels=commands,\n",
    "            annot=True, fmt='g')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQGi_mzPcLvl"
   },
   "source": [
    "## Run inference on an audio file\n",
    "\n",
    "Finally, verify the model's prediction output using an input audio file of someone saying \"no\". How well does your model perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-03T01:21:58.082021Z",
     "iopub.status.busy": "2022-09-03T01:21:58.081349Z",
     "iopub.status.idle": "2022-09-03T01:21:58.236705Z",
     "shell.execute_reply": "2022-09-03T01:21:58.236129Z"
    },
    "id": "zRxauKMdhofU"
   },
   "outputs": [],
   "source": [
    "x = data_dir/'no/01bb6a2a_nohash_0.wav'\n",
    "x = tf.io.read_file(str(x))\n",
    "x, sample_rate = tf.audio.decode_wav(x, desired_channels=1, desired_samples=16000,)\n",
    "x = tf.squeeze(x, axis=-1)\n",
    "waveform = x\n",
    "x = get_spectrogram(x)\n",
    "x = x[tf.newaxis,...]\n",
    "\n",
    "prediction = model(x)\n",
    "plt.bar(commands, tf.nn.softmax(prediction[0]))\n",
    "plt.title('No')\n",
    "plt.show()\n",
    "\n",
    "display.display(display.Audio(waveform, rate=16000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgWICqdqQNaQ"
   },
   "source": [
    "As the output suggests, your model should have recognized the audio command as \"no\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1icqlM3ISW0"
   },
   "source": [
    "## Export the model with preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7HX-MjgIbji"
   },
   "source": [
    "The model's not very easy to use if you have to apply those preprocessing steps before passing data to the model for inference. So build an end-to-end version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-03T01:21:58.240485Z",
     "iopub.status.busy": "2022-09-03T01:21:58.240239Z",
     "iopub.status.idle": "2022-09-03T01:21:58.246477Z",
     "shell.execute_reply": "2022-09-03T01:21:58.245935Z"
    },
    "id": "2lIeXdWjIbDE"
   },
   "outputs": [],
   "source": [
    "class ExportModel(tf.Module):\n",
    "  def __init__(self, model):\n",
    "    self.model = model\n",
    "\n",
    "    # Accept either a string-filename or a batch of waveforms.\n",
    "    # YOu could add additional signatures for a single wave, or a ragged-batch. \n",
    "    self.__call__.get_concrete_function(\n",
    "        x=tf.TensorSpec(shape=(), dtype=tf.string))\n",
    "    self.__call__.get_concrete_function(\n",
    "       x=tf.TensorSpec(shape=[None, 16000], dtype=tf.float32))\n",
    "\n",
    "\n",
    "  @tf.function\n",
    "  def __call__(self, x):\n",
    "    # If they pass a string, load the file and decode it. \n",
    "    if x.dtype == tf.string:\n",
    "      x = tf.io.read_file(x)\n",
    "      x, _ = tf.audio.decode_wav(x, desired_channels=1, desired_samples=16000,)\n",
    "      x = tf.squeeze(x, axis=-1)\n",
    "      x = x[tf.newaxis, :]\n",
    "    \n",
    "    x = get_spectrogram(x)  \n",
    "    result = self.model(x, training=False)\n",
    "    \n",
    "    class_ids = tf.argmax(result, axis=-1)\n",
    "    class_names = tf.gather(label_names, class_ids)\n",
    "    return {'predictions':result,\n",
    "            'class_ids': class_ids,\n",
    "            'class_names': class_names}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtZBmUiB9HGY"
   },
   "source": [
    "Test run the \"export\" model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-03T01:21:58.250013Z",
     "iopub.status.busy": "2022-09-03T01:21:58.249401Z",
     "iopub.status.idle": "2022-09-03T01:21:58.533327Z",
     "shell.execute_reply": "2022-09-03T01:21:58.532662Z"
    },
    "id": "Z1_8TYaCIRue"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m export \u001b[38;5;241m=\u001b[39m ExportModel(\u001b[43mmodel\u001b[49m)\n\u001b[0;32m      2\u001b[0m export(tf\u001b[38;5;241m.\u001b[39mconstant(\u001b[38;5;28mstr\u001b[39m(data_dir\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno/01bb6a2a_nohash_0.wav\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "export = ExportModel(model)\n",
    "export(tf.constant(str(data_dir/'no/01bb6a2a_nohash_0.wav')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1J6Iuz829Cxo"
   },
   "source": [
    "Save and reload the model, the reloaded model gives identical output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-03T01:21:58.537104Z",
     "iopub.status.busy": "2022-09-03T01:21:58.536628Z",
     "iopub.status.idle": "2022-09-03T01:22:00.126338Z",
     "shell.execute_reply": "2022-09-03T01:22:00.125688Z"
    },
    "id": "wTAg4vsn3oEb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "  WARNING: Failed to write executable - trying to use .deleteme logic\n",
      "ERROR: Could not install packages due to an OSError: [WinError 2] The system cannot find the file specified: 'c:\\\\python39\\\\Scripts\\\\tensorboard.exe' -> 'c:\\\\python39\\\\Scripts\\\\tensorboard.exe.deleteme'\n",
      "\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.3.1; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (2.9.1)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.10.0-cp39-cp39-win_amd64.whl (455.9 MB)\n",
      "Collecting tensorflow_datasets\n",
      "  Using cached tensorflow_datasets-4.6.0-py3-none-any.whl (4.3 MB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\python39\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (3.19.4)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (0.4.0)\n",
      "Collecting keras<2.11,>=2.10.0\n",
      "  Using cached keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Collecting tensorflow-estimator<2.11,>=2.10.0\n",
      "  Using cached tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\python39\\lib\\site-packages (from tensorflow) (1.21.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (0.2.0)\n",
      "Collecting tensorboard<2.11,>=2.10\n",
      "  Using cached tensorboard-2.10.0-py3-none-any.whl (5.9 MB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: setuptools in c:\\python39\\lib\\site-packages (from tensorflow) (56.0.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (0.26.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (14.0.1)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Using cached flatbuffers-2.0.7-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: packaging in c:\\python39\\lib\\site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (1.46.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\python39\\lib\\site-packages (from tensorflow) (4.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (1.1.0)\n",
      "Collecting dill\n",
      "  Using cached dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
      "Collecting promise\n",
      "  Using cached promise-2.3-py3-none-any.whl\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow_datasets) (2.26.0)\n",
      "Collecting toml\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: tqdm in c:\\python39\\lib\\site-packages (from tensorflow_datasets) (4.64.1)\n",
      "Collecting tensorflow-metadata\n",
      "  Using cached tensorflow_metadata-1.10.0-py3-none-any.whl (50 kB)\n",
      "Requirement already satisfied: etils[epath] in c:\\python39\\lib\\site-packages (from tensorflow_datasets) (0.8.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python39\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\python39\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python39\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (3.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\python39\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.0.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.7.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: importlib_resources in c:\\python39\\lib\\site-packages (from etils[epath]->tensorflow_datasets) (5.9.0)\n",
      "Requirement already satisfied: zipp in c:\\python39\\lib\\site-packages (from etils[epath]->tensorflow_datasets) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\python39\\lib\\site-packages (from packaging->tensorflow) (3.0.6)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in c:\\python39\\lib\\site-packages (from tensorflow-metadata->tensorflow_datasets) (1.56.4)\n",
      "Requirement already satisfied: colorama in c:\\python39\\lib\\site-packages (from tqdm->tensorflow_datasets) (0.4.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\python39\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (4.11.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\python39\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (3.2.0)\n",
      "Installing collected packages: toml, tensorflow-metadata, tensorflow-estimator, tensorboard, promise, keras, flatbuffers, dill, tensorflow-datasets, tensorflow\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.9.0\n",
      "    Uninstalling tensorflow-estimator-2.9.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.9.1\n",
      "    Uninstalling tensorboard-2.9.1:\n",
      "      Successfully uninstalled tensorboard-2.9.1\n",
      "  Rolling back uninstall of tensorboard\n",
      "  Moving to c:\\users\\user\\appdata\\roaming\\python\\python39\\scripts\\tensorboard.exe\n",
      "   from C:\\Users\\User\\AppData\\Local\\Temp\\pip-uninstall-atxpq3xj\\tensorboard.exe\n",
      "  Moving to c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages\\tensorboard-2.9.1.dist-info\\\n",
      "   from C:\\Users\\User\\AppData\\Roaming\\Python\\Python39\\site-packages\\~ensorboard-2.9.1.dist-info\n",
      "  Moving to c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages\\tensorboard\\\n",
      "   from C:\\Users\\User\\AppData\\Roaming\\Python\\Python39\\site-packages\\~ensorboard\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(export, \"saved\")\n",
    "imported = tf.saved_model.load(\"saved\")\n",
    "imported(waveform[tf.newaxis, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3jF933m9z1J"
   },
   "source": [
    "## Next steps\n",
    "\n",
    "This tutorial demonstrated how to carry out simple audio classification/automatic speech recognition using a convolutional neural network with TensorFlow and Python. To learn more, consider the following resources:\n",
    "\n",
    "- The [Sound classification with YAMNet](https://www.tensorflow.org/hub/tutorials/yamnet) tutorial shows how to use transfer learning for audio classification.\n",
    "- The notebooks from <a href=\"https://www.kaggle.com/c/tensorflow-speech-recognition-challenge/overview\" class=\"external\">Kaggle's TensorFlow speech recognition challenge</a>.\n",
    "- The \n",
    "<a href=\"https://codelabs.developers.google.com/codelabs/tensorflowjs-audio-codelab/index.html#0\" class=\"external\">TensorFlow.js - Audio recognition using transfer learning codelab</a> teaches how to build your own interactive web app for audio classification.\n",
    "- <a href=\"https://arxiv.org/abs/1709.04396\" class=\"external\">A tutorial on deep learning for music information retrieval</a> (Choi et al., 2017) on arXiv.\n",
    "- TensorFlow also has additional support for [audio data preparation and augmentation](https://www.tensorflow.org/io/tutorials/audio) to help with your own audio-based projects.\n",
    "- Consider using the <a href=\"https://librosa.org/\" class=\"external\">librosa</a> library—a Python package for music and audio analysis."
   ]
  }
 ],
 "metadata": {
  "accelerator": "CPU",
  "colab": {
   "collapsed_sections": [],
   "name": "simple_audio.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "f544ce1a915a9875fad91c894e2c0bcad4b7a79945aa6027ef3ad27810072aa6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
